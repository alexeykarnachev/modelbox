services:
  triton-server:
    image: nvcr.io/nvidia/tritonserver:24.12-py3
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    ports:
      - "8000:8000"  # HTTP
      - "8001:8001"  # gRPC
      - "8002:8002"  # Metrics
    volumes:
      - ./model_repository:/models
    command: ["tritonserver", "--model-repository=/models"]
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
    shm_size: "1g"
    restart: unless-stopped

  triton-web-ui:
    image: duyvd/triton-inference-server-web-ui
    ports:
      - "3000:3000"
    environment:
      - API_URL=http://triton-server:8000
    depends_on:
      - triton-server
    restart: unless-stopped

  litestar-api:
    build:
      context: .
      dockerfile: Dockerfile
    ports:
      - "8228:8228"
    depends_on:
      - triton-server
    restart: unless-stopped
